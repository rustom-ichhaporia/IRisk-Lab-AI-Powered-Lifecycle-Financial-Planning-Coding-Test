{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mortality Group Weekly Report - 11/7/2020\n",
    "Rustom Ichhaporia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import lightgbm as lgb\n",
    "import csv\n",
    "from hyperopt import STATUS_OK, hp, tpe, Trials, fmin\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "from timeit import default_timer as timer\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         record  age  race  sex   ms  hisp  adjinc  educ  pob   wt  ...   vt  \\\n",
       "0         88426   70   1.0    2  5.0   3.0    11.0   4.0  909  151  ...  0.0   \n",
       "1         88427   79   1.0    2  2.0   3.0    11.0   4.0  909  132  ...  0.0   \n",
       "2         88428   34   1.0    1  1.0   3.0     8.0   4.0  909  155  ...  0.0   \n",
       "3         88429   32   1.0    2  1.0   3.0     8.0   1.0  909  155  ...  0.0   \n",
       "4         88430    2   1.0    2  NaN   3.0     8.0   NaN  909  145  ...  NaN   \n",
       "...         ...  ...   ...  ...  ...   ...     ...   ...  ...  ...  ...  ...   \n",
       "1835067     666   19   1.0    1  5.0   2.0     4.0   8.0  909   60  ...  0.0   \n",
       "1835068     667   33   1.0    2  1.0   2.0    11.0   6.0  909   56  ...  0.0   \n",
       "1835069     668   16   1.0    2  5.0   2.0    11.0   6.0  909   60  ...  0.0   \n",
       "1835070     669    7   1.0    2  NaN   2.0    11.0   NaN  909   51  ...  NaN   \n",
       "1835071     670    6   1.0    1  NaN   2.0    11.0   NaN  909   56  ...  NaN   \n",
       "\n",
       "         histatus  hitype  povpct  stater  rcow  tenure  citizen  health  \\\n",
       "0             NaN     NaN      18      16   4.0     1.0      NaN     NaN   \n",
       "1             NaN     NaN      18      16   3.0     1.0      NaN     NaN   \n",
       "2             NaN     NaN      10      16   1.0     2.0      NaN     NaN   \n",
       "3             NaN     NaN      10      16   1.0     2.0      NaN     NaN   \n",
       "4             NaN     NaN      10      16   NaN     2.0      NaN     NaN   \n",
       "...           ...     ...     ...     ...   ...     ...      ...     ...   \n",
       "1835067       1.0     4.0       6      16   2.0     2.0      NaN     1.0   \n",
       "1835068       1.0     4.0      10      16   NaN     2.0      1.0     1.0   \n",
       "1835069       1.0     4.0      10      16   NaN     2.0      NaN     1.0   \n",
       "1835070       1.0     4.0      10      16   NaN     2.0      1.0     1.0   \n",
       "1835071       1.0     4.0      10      16   NaN     2.0      NaN     1.0   \n",
       "\n",
       "         indalg  \n",
       "0           1.0  \n",
       "1           NaN  \n",
       "2           1.0  \n",
       "3           1.0  \n",
       "4           1.0  \n",
       "...         ...  \n",
       "1835067     NaN  \n",
       "1835068     NaN  \n",
       "1835069     NaN  \n",
       "1835070     NaN  \n",
       "1835071     NaN  \n",
       "\n",
       "[1835072 rows x 37 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>record</th>\n      <th>age</th>\n      <th>race</th>\n      <th>sex</th>\n      <th>ms</th>\n      <th>hisp</th>\n      <th>adjinc</th>\n      <th>educ</th>\n      <th>pob</th>\n      <th>wt</th>\n      <th>...</th>\n      <th>vt</th>\n      <th>histatus</th>\n      <th>hitype</th>\n      <th>povpct</th>\n      <th>stater</th>\n      <th>rcow</th>\n      <th>tenure</th>\n      <th>citizen</th>\n      <th>health</th>\n      <th>indalg</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>88426</td>\n      <td>70</td>\n      <td>1.0</td>\n      <td>2</td>\n      <td>5.0</td>\n      <td>3.0</td>\n      <td>11.0</td>\n      <td>4.0</td>\n      <td>909</td>\n      <td>151</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>18</td>\n      <td>16</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>88427</td>\n      <td>79</td>\n      <td>1.0</td>\n      <td>2</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>11.0</td>\n      <td>4.0</td>\n      <td>909</td>\n      <td>132</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>18</td>\n      <td>16</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>88428</td>\n      <td>34</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>8.0</td>\n      <td>4.0</td>\n      <td>909</td>\n      <td>155</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>10</td>\n      <td>16</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>88429</td>\n      <td>32</td>\n      <td>1.0</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>8.0</td>\n      <td>1.0</td>\n      <td>909</td>\n      <td>155</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>10</td>\n      <td>16</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>88430</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>3.0</td>\n      <td>8.0</td>\n      <td>NaN</td>\n      <td>909</td>\n      <td>145</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>10</td>\n      <td>16</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1835067</th>\n      <td>666</td>\n      <td>19</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>5.0</td>\n      <td>2.0</td>\n      <td>4.0</td>\n      <td>8.0</td>\n      <td>909</td>\n      <td>60</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>6</td>\n      <td>16</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1835068</th>\n      <td>667</td>\n      <td>33</td>\n      <td>1.0</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>11.0</td>\n      <td>6.0</td>\n      <td>909</td>\n      <td>56</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>10</td>\n      <td>16</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1835069</th>\n      <td>668</td>\n      <td>16</td>\n      <td>1.0</td>\n      <td>2</td>\n      <td>5.0</td>\n      <td>2.0</td>\n      <td>11.0</td>\n      <td>6.0</td>\n      <td>909</td>\n      <td>60</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>10</td>\n      <td>16</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1835070</th>\n      <td>669</td>\n      <td>7</td>\n      <td>1.0</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>11.0</td>\n      <td>NaN</td>\n      <td>909</td>\n      <td>51</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>10</td>\n      <td>16</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1835071</th>\n      <td>670</td>\n      <td>6</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>11.0</td>\n      <td>NaN</td>\n      <td>909</td>\n      <td>56</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>10</td>\n      <td>16</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>1835072 rows × 37 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "df_raw = pd.read_csv('data/11.csv').drop(['smok100', 'agesmk', 'smokstat', 'smokhome', 'curruse', 'everuse'], axis=1)\n",
    "df_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this data takes a long time to compile and verify from multiple smaller studies, the creators included both a definite and an algorithmic indicator of death to speed up the release of the data. These features, `'inddea'` and `'indalg'`, respectively, have been combined through intersection into one target variable, `'indmort'` in the way that the reference manual recommends. The expanded list of features is below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical = ['age', 'hhnum']\n",
    "uneven_numerical = ['adjinc', 'health', 'follow']\n",
    "categorical = ['race', 'sex', 'ms', 'hisp', 'educ', 'pob', 'hhid', 'reltrf', 'occ', 'majocc', 'ind', 'esr', 'urban', 'smsast', 'inddea', 'cause113', 'dayod', 'hosp', 'hospd', 'ssnyn', 'vt', 'histatus', 'hitype', 'povpct', 'stater', 'rcow', 'tenure', 'citizen', 'indalg']\n",
    "smoking = ['smok100', 'agesmk', 'smokstat', 'smokhome', 'curruse', 'everuse']\n",
    "misc = ['record', 'wt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<pre>List of all the features and their full names is pasted below. For the full description of the features, refer to the read_pubfile5.dat file. \n",
    "@1 record     $   7.     /*Record Number (page no. 6)                              */\n",
    "    @8 age            2.     /*Age at Time of Interview (page no. 10)                  */\n",
    "    @10 race      $   1.     /*Race  (page no.12)                                      */\n",
    "    @11 sex       $   1.     /*Sex   (page no.10)                                      */\n",
    "    @12 ms        $   1.     /*Marital Status (page no.13)                             */\n",
    "    @13 hisp      $   1.     /*Hispanic Origin (page no. 12)                           */\n",
    "    @14 adjinc    $   2.     /*Inflation Adjusted Income (page no.20)                  */\n",
    "    @16 educ      $   2.     /*Highest Grade Completed (page no.14)                    */\n",
    "    @18 pob       $   3.     /*Region of Birth (page no. 11)                           */\n",
    "    @21 wt            4.     /*Adjusted Weight (page no. 6 )                           */\n",
    "    @25 hhid      $   7.     /*Household ID No. (page no. 6)                           */\n",
    "    @32 hhnum         2.     /*Number of People in HH (page no. 14)                    */\n",
    "    @34 reltrf    $   1.     /*Relationship to Reference Person (page no.13)           */\n",
    "    @35 occ       $   4.     /*4 Digit Occupation Code (page no. 18)                   */\n",
    "    @39 majocc    $   2.     /*Major Occupation Code (page no. 18 )                    */\n",
    "    @41 ind       $   4.     /*4 Digit Industry Code (page no. 17)                     */\n",
    "    @45 majind    $   2.     /*Major Industry Code (page no. 18)                       */\n",
    "    @47 esr       $   1.     /*Employment Status Recode (page no. 17)                  */\n",
    "    @48 urban     $   1.     /*Urban/Rural Status (page no. 8)                         */\n",
    "    @49 smsast    $   1.     /*SMSAST Status (page no.9)                               */\n",
    "    @50 inddea    $   1.     /*Death Indicator (page no. 23)                           */\n",
    "    @51 cause113  $   3.     /*Cause of Death (page no. 23)                            */\n",
    "    @54 follow        4.     /*Length of Follow-up (page no. 24)                       */\n",
    "    @58 dayod     $   1.     /*Day of Week of Death (page no. 24)                      */\n",
    "    @59 hosp      $   1.     /*Hospital Type (page no.25)                              */\n",
    "    @60 hospd     $   1.     /*Hospital Death Indicator (page no. 26)                  */\n",
    "    @61 ssnyn     $   1.     /*Presence of SSN (page no. 7)                            */\n",
    "    @62 vt        $   1.     /*Veteran Status (page no. 16)                            */\n",
    "    @63 histatus  $   1.     /*Health Insurance Status (page no. 22)                   */\n",
    "    @64 hitype    $   1.     /*Health Insurance Type (page no. 22)                     */\n",
    "    @65 povpct    $   2.     /*Income as Percent of Poverty Level (page no. 21)        */\n",
    "    @67 stater    $   2.     /*State Recode (page no. 8)                               */\n",
    "    @69 rcow      $   2.     /*Recoded Class of Worker (page no.19)                    */\n",
    "    @71 tenure    $   1.     /*Housing Tenure (page no. 14)                            */\n",
    "    @72 citizen   $   1.     /*Citizenship (page no. 16)                               */\n",
    "    @73 health    $   2.     /*Health (page no. 16)                                    */\n",
    "    @75 indalg        1.     /*Indicator of Algorithmic Death (page no. 27)            */\n",
    "    @76 smok100   $   1.     /*Smoked More than 100 Cigarettes (page no. 28)           */\n",
    "    @77 agesmk    $   2.     /*Age Started Smoking (page no. 28)                       */\n",
    "    @79 smokstat  $   1.     /*Cigarette Smoking Status (page no.28)                   */\n",
    "    @80 smokhome  $   1.     /*Rules for Smoking Cigarettes in the Home (page no. 29 ) */\n",
    "    @81 curruse   $   5.     /*Currently Use Smokeless Tobacco (page no. 30)           */\n",
    "    @86 everuse   $   5.     /*Ever Use Smokeless Tobacco (page no. 31)                */</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "94107.0"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# indmort is the recommended combination feature of both confirmed deaths and computer-predicted deaths based on the data collection agency\n",
    "df_raw['indmort'] = df_raw['inddea'][(df_raw['inddea'] == 1) & (df_raw['indalg'] == 1)]\n",
    "df_raw['indmort'] = df_raw['indmort'].fillna(0)\n",
    "df_raw['indmort'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection of fewer variables for EDA purposes\n",
    "# Remove cause113 because it is not predictive\n",
    "used_features = ['age', 'hhnum', 'adjinc', 'health', 'occ', 'ind', 'esr', 'ms', 'indmort']\n",
    "df = df_raw[used_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "age        0.000000\n",
       "hhnum      0.000000\n",
       "adjinc     0.024124\n",
       "health     0.790674\n",
       "occ        0.466099\n",
       "ind        0.466219\n",
       "esr        0.191220\n",
       "ms         0.196846\n",
       "indmort    0.000000\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "df.isna().sum() / df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "hhnum     -0.169388\n",
       "adjinc    -0.098752\n",
       "ms        -0.073020\n",
       "ind       -0.010118\n",
       "occ        0.004227\n",
       "esr        0.195555\n",
       "health     0.282516\n",
       "age        0.336753\n",
       "indmort    1.000000\n",
       "Name: indmort, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "mort_corr = df.corr()['indmort'].sort_values()\n",
    "mort_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlations can be useful for numerical features, but most of these features are categorical, so I decided to begin one-hot encoding and imputation of missing values. Most of the health rating entries are still missing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "age           int64\n",
       "hhnum         int64\n",
       "adjinc      float64\n",
       "health      float64\n",
       "occ        category\n",
       "ind        category\n",
       "esr        category\n",
       "ms         category\n",
       "indmort     float64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "df = df.astype({'occ':'category', 'ind': 'category', 'esr': 'category', 'ms': 'category'})\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.get_dummies(df, dummy_na=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I still am not really sure what the best way for me to impute the values is as I have not really been able to discover this from looking at the reports of other groups. However, after doing some of my own research, I found that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         age  hhnum  adjinc  health     occ     ind  esr   ms  indmort\n",
       "0         70      2    11.0     NaN  2630.0  5470.0  1.0  5.0      0.0\n",
       "1         79      2    11.0     NaN  4700.0  5470.0  1.0  2.0      0.0\n",
       "2         34      3     8.0     NaN  8960.0  2980.0  1.0  1.0      0.0\n",
       "3         32      3     8.0     NaN  8960.0  5470.0  1.0  1.0      0.0\n",
       "4          2      3     8.0     NaN     NaN     NaN  NaN  NaN      0.0\n",
       "...      ...    ...     ...     ...     ...     ...  ...  ...      ...\n",
       "1835067   19      2     4.0     1.0  4760.0  4770.0  1.0  5.0      0.0\n",
       "1835068   33      6    11.0     1.0     NaN     NaN  5.0  1.0      0.0\n",
       "1835069   16      6    11.0     1.0     NaN     NaN  5.0  5.0      0.0\n",
       "1835070    7      6    11.0     1.0     NaN     NaN  NaN  NaN      0.0\n",
       "1835071    6      6    11.0     1.0     NaN     NaN  NaN  NaN      0.0\n",
       "\n",
       "[1835072 rows x 9 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>hhnum</th>\n      <th>adjinc</th>\n      <th>health</th>\n      <th>occ</th>\n      <th>ind</th>\n      <th>esr</th>\n      <th>ms</th>\n      <th>indmort</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>70</td>\n      <td>2</td>\n      <td>11.0</td>\n      <td>NaN</td>\n      <td>2630.0</td>\n      <td>5470.0</td>\n      <td>1.0</td>\n      <td>5.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>79</td>\n      <td>2</td>\n      <td>11.0</td>\n      <td>NaN</td>\n      <td>4700.0</td>\n      <td>5470.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>34</td>\n      <td>3</td>\n      <td>8.0</td>\n      <td>NaN</td>\n      <td>8960.0</td>\n      <td>2980.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>32</td>\n      <td>3</td>\n      <td>8.0</td>\n      <td>NaN</td>\n      <td>8960.0</td>\n      <td>5470.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>3</td>\n      <td>8.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1835067</th>\n      <td>19</td>\n      <td>2</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>4760.0</td>\n      <td>4770.0</td>\n      <td>1.0</td>\n      <td>5.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1835068</th>\n      <td>33</td>\n      <td>6</td>\n      <td>11.0</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>5.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1835069</th>\n      <td>16</td>\n      <td>6</td>\n      <td>11.0</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1835070</th>\n      <td>7</td>\n      <td>6</td>\n      <td>11.0</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1835071</th>\n      <td>6</td>\n      <td>6</td>\n      <td>11.0</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1835072 rows × 9 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "NaN    1370211\n",
       "1.0     406059\n",
       "5.0      34765\n",
       "4.0      16009\n",
       "3.0       4167\n",
       "2.0       3861\n",
       "Name: citizen, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "df_raw['citizen'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "NaN    1677278\n",
       "1.0      91819\n",
       "4.0      34968\n",
       "2.0      25142\n",
       "5.0       5308\n",
       "6.0        387\n",
       "3.0        170\n",
       "Name: hosp, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "df_raw['hosp'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "record      0.000000\n",
       "stater      0.000000\n",
       "povpct      0.000000\n",
       "ssnyn       0.000000\n",
       "follow      0.000000\n",
       "cause113    0.000000\n",
       "inddea      0.000000\n",
       "hhnum       0.000000\n",
       "hhid        0.000000\n",
       "indmort     0.000000\n",
       "pob         0.000000\n",
       "age         0.000000\n",
       "sex         0.000000\n",
       "wt          0.000000\n",
       "race        0.001575\n",
       "reltrf      0.002547\n",
       "urban       0.007107\n",
       "smsast      0.007115\n",
       "tenure      0.013867\n",
       "adjinc      0.024124\n",
       "hisp        0.028610\n",
       "educ        0.191202\n",
       "esr         0.191220\n",
       "ms          0.196846\n",
       "vt          0.215823\n",
       "histatus    0.314846\n",
       "hitype      0.314846\n",
       "rcow        0.465219\n",
       "majocc      0.466099\n",
       "occ         0.466099\n",
       "majind      0.466219\n",
       "ind         0.466219\n",
       "citizen     0.746680\n",
       "health      0.790674\n",
       "indalg      0.809948\n",
       "dayod       0.912401\n",
       "hosp        0.914012\n",
       "hospd       0.920889\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "(df_raw.isna().sum() / df_raw.shape[0]).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "94107.0"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# indmort is the recommended combination feature of both confirmed deaths and computer-predicted deaths based on the data collection agency\n",
    "df_raw['indmort'] = df_raw['inddea'][(df_raw['inddea'] == 1) & (df_raw['indalg'] == 1)]\n",
    "df_raw['indmort'] = df_raw['indmort'].fillna(0)\n",
    "df_raw['indmort'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "age          int64\n",
       "sex          int64\n",
       "stater       int64\n",
       "povpct       int64\n",
       "pob          int64\n",
       "race       float64\n",
       "urban      float64\n",
       "ms         float64\n",
       "adjinc     float64\n",
       "educ       float64\n",
       "indmort    float64\n",
       "wt           int64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "numerical = ['age', 'hhnum', 'povpct']\n",
    "uneven_numerical = ['adjinc', 'health', 'follow']\n",
    "categorical = ['race', 'sex', 'ms', 'hisp', 'educ', 'pob', 'hhid', 'reltrf', 'occ', 'majocc', 'ind', 'esr', 'urban', 'smsast', 'inddea', 'cause113', 'dayod', 'hosp', 'hospd', 'ssnyn', 'vt', 'histatus', 'hitype', 'povpct', 'stater', 'rcow', 'tenure', 'citizen', 'indalg']\n",
    "smoking = ['smok100', 'agesmk', 'smokstat', 'smokhome', 'curruse', 'everuse']\n",
    "misc = ['record', 'wt']\n",
    "\n",
    "used_features = ['age', 'sex', 'stater', 'povpct', 'pob', 'race', 'urban', 'ms', 'adjinc', 'educ', 'indmort', 'wt']#, 'educ', 'stater']#, 'wt']\n",
    "df = df_raw[used_features]\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "age           int64\n",
       "sex        category\n",
       "stater     category\n",
       "povpct        int64\n",
       "pob        category\n",
       "race       category\n",
       "urban      category\n",
       "ms         category\n",
       "adjinc      float64\n",
       "educ       category\n",
       "indmort     float64\n",
       "wt            int64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "df = df.astype({'sex':'category', 'stater': 'category', 'pob': 'category', 'race': 'category', 'urban': 'category', 'ms': 'category'})\n",
    "df = df.astype({'educ':'category', 'stater':'category'})\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.0    0.948718\n",
       "1.0    0.051282\n",
       "Name: indmort, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "df['indmort'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1835072, 12)"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "age        0.000000\n",
       "sex        0.000000\n",
       "stater     0.000000\n",
       "povpct     0.000000\n",
       "pob        0.000000\n",
       "indmort    0.000000\n",
       "wt         0.000000\n",
       "race       0.001575\n",
       "urban      0.007107\n",
       "adjinc     0.024124\n",
       "educ       0.191202\n",
       "ms         0.196846\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "(df.isna().sum() / df.shape[0]).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped = df.dropna(axis=0)\n",
    "df_dropped = pd.get_dummies(df_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1443262, 147)"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "df_dropped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dead / alive ratio before SMOTE: 0.06430156132427792\n"
     ]
    }
   ],
   "source": [
    "y = df_dropped['indmort']\n",
    "X = df_dropped.drop(columns=['indmort'])\n",
    "print('Dead / alive ratio before SMOTE:', y.sum() / y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dead / alive ratio after SMOTE: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Only use oversampling for the train dataset, use regular distribution for testing\n",
    "oversample = SMOTE()\n",
    "X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
    "print('Dead / alive ratio after SMOTE:', y_train.sum() / y_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = lgb.Dataset(X_train, label=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below code is from Jiaxin and Jingbin \n",
    "\n",
    "MAX_EVALS = 5\n",
    "N_FOLDS = 3\n",
    "\n",
    "def objective(params, n_folds = N_FOLDS):\n",
    "    \"\"\"Objective function for Gradient Boosting Machine Hyperparameter Optimization\"\"\"\n",
    "    \n",
    "    # Keep track of evals\n",
    "    global ITERATION\n",
    "    \n",
    "    ITERATION += 1\n",
    "    \n",
    "    # Retrieve the subsample if present otherwise set to 1.0\n",
    "    subsample = params['boosting_type'].get('subsample', 1.0)\n",
    "    \n",
    "    # Extract the boosting type\n",
    "    params['boosting_type'] = params['boosting_type']['boosting_type']\n",
    "    params['subsample'] = subsample\n",
    "    \n",
    "    # Make sure parameters that need to be integers are integers\n",
    "    for parameter_name in ['num_leaves', 'subsample_for_bin', 'min_child_samples']:\n",
    "        params[parameter_name] = int(params[parameter_name])\n",
    "    \n",
    "    start = timer()\n",
    "    \n",
    "    # Perform n_folds cross validation\n",
    "    cv_results = lgb.cv(params, train_set, num_boost_round = 1000, nfold = n_folds, \n",
    "                        early_stopping_rounds = 100, metrics = 'auc', seed = 50)\n",
    "    # print(cv_results)\n",
    "    \n",
    "    run_time = timer() - start\n",
    "    \n",
    "    # Extract the best score\n",
    "    # best_score = np.min(cv_results['mse-mean'])\n",
    "    best_score = np.max(cv_results['auc-mean'])\n",
    "    \n",
    "    # Loss must be minimized\n",
    "    loss = 1 - best_score\n",
    "    \n",
    "    # Boosting rounds that returned the lowest cv score\n",
    "    n_estimators = int(np.argmax(cv_results['auc-mean']) + 1)\n",
    "\n",
    "    # Write to the csv file ('a' means append)\n",
    "    of_connection = open(out_file, 'a')\n",
    "    writer = csv.writer(of_connection)\n",
    "    writer.writerow([loss, params, ITERATION, n_estimators, run_time])\n",
    "    \n",
    "    # Dictionary with information for evaluation\n",
    "    return {'loss': loss, 'params': params, 'iteration': ITERATION,\n",
    "            'estimators': n_estimators, \n",
    "            'train_time': run_time, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'boosting_type': 'gbdt',\n",
       " 'colsample_bytree': 0.9864416326585367,\n",
       " 'learning_rate': 0.060025627594363265,\n",
       " 'min_child_samples': 245.0,\n",
       " 'num_leaves': 75.0,\n",
       " 'reg_alpha': 0.06728569341468615,\n",
       " 'reg_lambda': 0.8998191545637946,\n",
       " 'subsample_for_bin': 220000.0,\n",
       " 'subsample': 0.551248433618183}"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "space = {\n",
    "    'boosting_type': hp.choice('boosting_type', [{'boosting_type': 'gbdt', 'subsample': hp.uniform('gdbt_subsample', 0.5, 1)}, \n",
    "                                                 {'boosting_type': 'dart', 'subsample': hp.uniform('dart_subsample', 0.5, 1)},\n",
    "                                                 {'boosting_type': 'goss', 'subsample': 1.0}]),\n",
    "    'num_leaves': hp.quniform('num_leaves', 30, 150, 1),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),\n",
    "    'subsample_for_bin': hp.quniform('subsample_for_bin', 20000, 300000, 20000),\n",
    "    'min_child_samples': hp.quniform('min_child_samples', 20, 500, 5),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n",
    "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.6, 1.0)\n",
    "}\n",
    "x = sample(space)\n",
    "\n",
    "# Conditional logic to assign top-level keys\n",
    "subsample = x['boosting_type'].get('subsample', 1.0)\n",
    "x['boosting_type'] = x['boosting_type']['boosting_type']\n",
    "x['subsample'] = subsample\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization algorithm\n",
    "tpe_algorithm = tpe.suggest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of results\n",
    "bayes_trials = Trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File to save first results\n",
    "out_file = 'gbm_trials.csv'\n",
    "of_connection = open(out_file, 'w')\n",
    "writer = csv.writer(of_connection)\n",
    "\n",
    "# Write the headers to the file\n",
    "writer.writerow(['loss', 'params', 'iteration', 'estimators', 'train_time'])\n",
    "of_connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "100%|██████████| 5/5 [11:17<00:00, 135.55s/trial, best loss: 0.009053589281652008]\n"
     ]
    }
   ],
   "source": [
    "global  ITERATION\n",
    "\n",
    "ITERATION = 0\n",
    "\n",
    "# Run optimization\n",
    "best = fmin(fn = objective, space = space, algo = tpe.suggest, \n",
    "            max_evals = MAX_EVALS, trials = bayes_trials, rstate = np.random.RandomState(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'loss': 0.009053589281652008,\n",
       "  'params': {'boosting_type': 'gbdt',\n",
       "   'colsample_bytree': 0.8970523178797932,\n",
       "   'learning_rate': 0.035742378218536576,\n",
       "   'min_child_samples': 45,\n",
       "   'num_leaves': 138,\n",
       "   'reg_alpha': 0.5005020213127344,\n",
       "   'reg_lambda': 0.45121616279208887,\n",
       "   'subsample_for_bin': 260000,\n",
       "   'subsample': 0.6614743075688195},\n",
       "  'iteration': 2,\n",
       "  'estimators': 1000,\n",
       "  'train_time': 299.79700322198914,\n",
       "  'status': 'ok'},\n",
       " {'loss': 0.010676149743638619,\n",
       "  'params': {'boosting_type': 'gbdt',\n",
       "   'colsample_bytree': 0.620649129448606,\n",
       "   'learning_rate': 0.017934544496484815,\n",
       "   'min_child_samples': 260,\n",
       "   'num_leaves': 30,\n",
       "   'reg_alpha': 0.2092981310373776,\n",
       "   'reg_lambda': 0.8946886330215067,\n",
       "   'subsample_for_bin': 20000,\n",
       "   'subsample': 0.8731487506937664},\n",
       "  'iteration': 5,\n",
       "  'estimators': 1000,\n",
       "  'train_time': 263.6446141789929,\n",
       "  'status': 'ok'}]"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "# Sort the trials with lowest loss (lowest MSE) first\n",
    "bayes_trials_results = sorted(bayes_trials.results, key = lambda x: x['loss'])\n",
    "bayes_trials_results[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       loss                                             params  iteration  \\\n",
       "0  0.009054  {'boosting_type': 'gbdt', 'colsample_bytree': ...          2   \n",
       "1  0.010676  {'boosting_type': 'gbdt', 'colsample_bytree': ...          5   \n",
       "2  0.022391  {'boosting_type': 'goss', 'colsample_bytree': ...          1   \n",
       "3  0.025200  {'boosting_type': 'goss', 'colsample_bytree': ...          3   \n",
       "4  0.026597  {'boosting_type': 'goss', 'colsample_bytree': ...          4   \n",
       "\n",
       "   estimators  train_time  \n",
       "0        1000  299.797003  \n",
       "1        1000  263.644614  \n",
       "2          53   48.269464  \n",
       "3          17   31.476822  \n",
       "4          12   34.531737  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loss</th>\n      <th>params</th>\n      <th>iteration</th>\n      <th>estimators</th>\n      <th>train_time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.009054</td>\n      <td>{'boosting_type': 'gbdt', 'colsample_bytree': ...</td>\n      <td>2</td>\n      <td>1000</td>\n      <td>299.797003</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.010676</td>\n      <td>{'boosting_type': 'gbdt', 'colsample_bytree': ...</td>\n      <td>5</td>\n      <td>1000</td>\n      <td>263.644614</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.022391</td>\n      <td>{'boosting_type': 'goss', 'colsample_bytree': ...</td>\n      <td>1</td>\n      <td>53</td>\n      <td>48.269464</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.025200</td>\n      <td>{'boosting_type': 'goss', 'colsample_bytree': ...</td>\n      <td>3</td>\n      <td>17</td>\n      <td>31.476822</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.026597</td>\n      <td>{'boosting_type': 'goss', 'colsample_bytree': ...</td>\n      <td>4</td>\n      <td>12</td>\n      <td>34.531737</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "results = pd.read_csv('gbm_trials.csv')\n",
    "\n",
    "# Sort with best scores on top and reset index for slicing\n",
    "results.sort_values('loss', ascending = True, inplace = True)\n",
    "results.reset_index(inplace = True, drop = True)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'boosting_type': 'gbdt',\n",
       " 'colsample_bytree': 0.8970523178797932,\n",
       " 'learning_rate': 0.035742378218536576,\n",
       " 'min_child_samples': 45,\n",
       " 'num_leaves': 138,\n",
       " 'reg_alpha': 0.5005020213127344,\n",
       " 'reg_lambda': 0.45121616279208887,\n",
       " 'subsample_for_bin': 260000,\n",
       " 'subsample': 0.6614743075688195}"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "# Convert from a string to a dictionary\n",
    "ast.literal_eval(results.loc[0, 'params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the ideal number of estimators and hyperparameters\n",
    "best_bayes_estimators = int(results.loc[0, 'estimators'])\n",
    "best_bayes_params = ast.literal_eval(results.loc[0, 'params']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1,data2 = train_test_split(df, train_size = 0.8, random_state = 42)\n",
    "n1 = data1.shape[0]\n",
    "data1.index=pd.Series(range(0,n1))\n",
    "n2 = data2.shape[0]\n",
    "data2.index=pd.Series(range(0,n2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data1.drop(['indmort'], axis=1)\n",
    "y_train = data1['indmort']\n",
    "X_test = data2.drop(['indmort'], axis=1)\n",
    "y_test = data2['indmort']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LGBMClassifier(colsample_bytree=0.8970523178797932,\n",
       "               learning_rate=0.035742378218536576, min_child_samples=45,\n",
       "               n_estimators=1000, num_leaves=138, objective='binary',\n",
       "               random_state=50, reg_alpha=0.5005020213127344,\n",
       "               reg_lambda=0.45121616279208887, subsample=0.6614743075688195,\n",
       "               subsample_for_bin=260000)"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "# Re-create the best model and train on the training data\n",
    "best_bayes_model = lgb.LGBMClassifier(n_estimators=best_bayes_estimators, n_jobs = -1, \n",
    "                                       objective = 'binary', random_state = 50, **best_bayes_params)\n",
    "best_bayes_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The best model from Bayes optimization scores 0.90207 AUC ROC on the test set.\n"
     ]
    }
   ],
   "source": [
    "preds = best_bayes_model.predict_proba(X_test)[:, 1]\n",
    "print('The best model from Bayes optimization scores {:.5f} AUC ROC on the test set.'.format(roc_auc_score(y_test, preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.00563657, 0.00359645, 0.0636843 , ..., 0.00532183, 0.04767225,\n",
       "       0.01777752])"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n         0.0       0.96      0.99      0.97    348185\n         1.0       0.59      0.16      0.25     18830\n\n    accuracy                           0.95    367015\n   macro avg       0.77      0.58      0.61    367015\nweighted avg       0.94      0.95      0.94    367015\n\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, preds.round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n         0.0       0.97      0.96      0.97    348185\n         1.0       0.40      0.48      0.44     18830\n\n    accuracy                           0.94    367015\n   macro avg       0.69      0.72      0.70    367015\nweighted avg       0.94      0.94      0.94    367015\n\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, (preds + 0.25).round()))"
   ]
  },
  {
   "source": [
    "The predictions are best when a constant is added to the final probabilities. While the ROC/AUC score of the model after parameter tuning jumps from 0.73 to 0.9, the precision and recall of the minority class are relatively unchanged. I tried changing the loss function to auc, but the results were almost identical. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_wt = X_train['wt']\n",
    "# X_train = X_train.drop(columns=['wt'])\n",
    "# X_test = X_test.drop(columns=['wt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LGBMClassifier(is_unbalance=True, max_depth=20)"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "model = lgb.LGBMClassifier(is_unbalance=True, max_depth=20)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n         0.0       0.97      0.92      0.94    337447\n         1.0       0.32      0.56      0.41     23369\n\n    accuracy                           0.89    360816\n   macro avg       0.64      0.74      0.67    360816\nweighted avg       0.93      0.89      0.91    360816\n\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7399945389401491"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "roc_auc_score(y_test, y_pred)\n",
    "# metrics.auc(fpr, tpr)"
   ]
  },
  {
   "source": [
    "estimators, "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average_precision_score(y_test, y_pred) # sample_weight=X_test.wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters = {\n",
    "#     \"loss\":[\"deviance\"],\n",
    "#     \"learning_rate\": [0.01],# , 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],\n",
    "#     # \"min_samples_split\": np.linspace(0.1, 0.5, 12),\n",
    "#     # \"min_samples_leaf\": np.linspace(0.1, 0.5, 12),\n",
    "#     \"max_depth\":[5],#[3,5,8],\n",
    "#     # \"max_features\":[\"log2\",\"sqrt\"],\n",
    "#     # \"criterion\": [\"roc\"],# [\"friedman_mse\",  \"mae\"],\n",
    "#     \"criterion\": [\"friedman_mse\"],\n",
    "#     # \"subsample\":[0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0],\n",
    "#     \"n_estimators\":[100]\n",
    "# }\n",
    "\n",
    "# model = GridSearchCV(GradientBoostingClassifier(), parameters, cv=10, n_jobs=-1)\n",
    "\n",
    "# model.fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}